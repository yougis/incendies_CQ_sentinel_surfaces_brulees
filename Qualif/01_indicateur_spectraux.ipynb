{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from intake import open_catalog\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "from pystac_client import Client as psc\n",
    "import stackstac\n",
    "import rasterio.features\n",
    "from pyproj import CRS\n",
    "from shapely.geometry import mapping\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "from dask.distributed import Client, LocalCluster, Variable\n",
    "import dask\n",
    "import dask_geopandas\n",
    "import xarray as xr\n",
    "from pathlib import Path\n",
    "import platform\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### dask worker tcp://172.20.12.11:8786 --nthreads=12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDaskClient(local=False):\n",
    "\n",
    "    def createClient(schedulerIp=\"172.20.12.11:8786\"):\n",
    "        client = Client(schedulerIp)\n",
    "        return client\n",
    "\n",
    "    if local :\n",
    "        # Démarrer un cluster local avec 4 cœurs\n",
    "        cluster = LocalCluster(n_workers=2,threads_per_worker=8,silence_logs='DEBUG',memory_limit='20GB', timeout=\"60s\",heartbeat_interval=\"10s\")\n",
    "\n",
    "        client = Client(cluster)\n",
    "        return client\n",
    "\n",
    "\n",
    "    if 'client' in globals():   \n",
    "        if client.scheduler != None:\n",
    "        # La variable client existe dans l'espace de noms global\n",
    "            return client\n",
    "        else:\n",
    "            schedulerIp = os.getenv(\"SCHEDULER_IP\")\n",
    "            if schedulerIp:\n",
    "                client = createClient(schedulerIp)\n",
    "            else:\n",
    "                client = createClient(schedulerIp)\n",
    "                \n",
    "    else:\n",
    "        schedulerIp = os.getenv(\"SCHEDULER_IP\")\n",
    "        if schedulerIp:\n",
    "            client = createClient(schedulerIp)\n",
    "        else:\n",
    "            client = createClient()\n",
    "\n",
    "    return client\n",
    "\n",
    "client = getDaskClient(local=False)\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_source = \"sentinel_surfaces_detectees\"\n",
    "\n",
    "date_start=datetime(2024,1,1)\n",
    "date_end=datetime(2024,5,31)\n",
    "\n",
    "URL = \"https://earth-search.aws.element84.com/v1\"\n",
    "URL_2 = \"https://catalogue.dataspace.copernicus.eu/stac\"\n",
    "URL_3 = \"https://services.sentinel-hub.com/api/v1/catalog/1.0.0/\"\n",
    "URL_4 = \"https://earthengine.openeo.org/v1.0/\"\n",
    "\n",
    "collection = \"SENTINEL-2\"\n",
    "collect_amazon ='sentinel-2-l2a'\n",
    "collect_EE = 'COPERNICUS/S2_SR_HARMONIZED'\n",
    "\n",
    "interval_before=150\n",
    "interval_after=40\n",
    "\n",
    "offset = 0\n",
    "limit = 12\n",
    "\n",
    "output_path = \"A:/INDICATEUR_FEUX/2024/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog_path = f'{os.getenv(\"PROJECT_PATH\")}/Fire_Detection_Data_Quality.yaml'\n",
    "\n",
    "sql = f\"\"\"SELECT row_number() OVER () AS id,\n",
    "si.date_,\n",
    "si.surface_id_h3,\n",
    "si.geometry\n",
    "FROM feux_cq.{table_source} si\n",
    "WHERE si.date_ >= '{pd.to_datetime(date_start).strftime('%Y-%m-%d')}' AND si.date_ <= '{pd.to_datetime(date_end).strftime('%Y-%m-%d')}'\n",
    "\"\"\"\n",
    "\n",
    "catalog = open_catalog(catalog_path)\n",
    "dataCatalog = getattr(catalog, table_source)(sql_expr=sql)\n",
    "gdf = dataCatalog.read()\n",
    "\n",
    "if gdf.duplicated(subset=['surface_id_h3']).sum()>=1:\n",
    "    gdf = gdf.drop_duplicates(subset=['surface_id_h3'])\n",
    "\n",
    "gdf = gdf.to_crs(epsg=4326)\n",
    "\n",
    "gdf['date_']= pd.to_datetime(gdf['date_'], format='%Y-%m-%d').dt.date\n",
    "gdf=gdf.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fichiers = [f for f in os.listdir(output_path) if os.path.isfile(os.path.join(output_path, f))]\n",
    "files=[]\n",
    "for fichier in fichiers:\n",
    "    files.append(fichier[:35])\n",
    "\n",
    "gdf = gdf[~gdf['surface_id_h3'].isin(files)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_indicateur_spectraux(data_indices):\n",
    "    \"\"\"\n",
    "    Correction des valeurs de bandes, on avait des valeurs négatives de reflectance par endroit\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    data_indices[\"red+\"]=data_indices[\"red\"].where(lambda x : x>0, lambda x : -x)\n",
    "    data_indices[\"nir+\"]=data_indices[\"nir\"].where(lambda x : x>0, lambda x : -x)\n",
    "    data_indices[\"swir22+\"]=data_indices[\"swir22\"].where(lambda x : x>0, lambda x : -x)\n",
    "    data_indices[\"swir16+\"]=data_indices[\"swir16\"].where(lambda x : x>0, lambda x : -x)\n",
    "    data_indices[\"green+\"]=data_indices[\"green\"].where(lambda x : x>0, lambda x : -x)\n",
    "    data_indices[\"blue+\"]=data_indices[\"blue\"].where(lambda x : x>0, lambda x : -x)\n",
    "    data_indices[\"nir08+\"]=data_indices[\"nir08\"].where(lambda x : x>0, lambda x : -x)\n",
    "    data_indices[\"rededge1+\"]=data_indices[\"rededge1\"].where(lambda x : x>0, lambda x : -x)\n",
    "    data_indices[\"rededge2+\"]=data_indices[\"rededge2\"].where(lambda x : x>0, lambda x : -x)\n",
    "    data_indices[\"rededge3+\"]=data_indices[\"rededge3\"].where(lambda x : x>0, lambda x : -x)\n",
    "    \"\"\"\n",
    "    data_indices[\"red\"] = data_indices[\"red\"]+0.1  #B4\n",
    "    data_indices[\"nir\"] = data_indices[\"nir\"]+0.1  #B8\n",
    "    data_indices[\"swir22\"] = data_indices[\"swir22\"]+0.1  #B12\n",
    "    data_indices[\"swir16\"] = data_indices[\"swir16\"]+0.1  #B11\n",
    "    data_indices[\"green\"] = data_indices[\"green\"]+0.1 #B3\n",
    "    data_indices[\"blue\"] = data_indices[\"blue\"]+0.1\n",
    "    data_indices[\"nir08\"] = data_indices[\"nir08\"]+0.1  #B8A\n",
    "    data_indices[\"rededge1\"] = data_indices[\"rededge1\"]+0.1  #B5\n",
    "    data_indices[\"rededge2\"] = data_indices[\"rededge2\"]+0.1   #B6\n",
    "    data_indices[\"rededge3\"] = data_indices[\"rededge3\"]+0.1   #B7\n",
    "\n",
    "    data_indices['ndvi'] = ((data_indices['nir'].astype(float) - data_indices['red'].astype(float))/(data_indices['nir'].astype(float)+ data_indices['red'].astype(float)))\n",
    "\n",
    "    data_indices['nbr'] = ((data_indices['nir'] - data_indices['swir22'])/(data_indices['nir'] + data_indices['swir22']))\n",
    "\n",
    "    #data_indices['nbr+'] = ((data_indices['swir22'] - data_indices['nir08'] - data_indices['green'] - data_indices['blue'])/(data_indices['swir22'] + data_indices['nir08'] + data_indices['green'] + data_indices['blue']))\n",
    "\n",
    "    data_indices['bais2'] = (1-(np.sqrt((data_indices['rededge2'] * data_indices['rededge3'] * data_indices['nir08'])/data_indices['red']))*((data_indices['swir22'] - data_indices['nir08'] )/ np.sqrt((data_indices['swir22'] + data_indices['nir08'] ))+1))\n",
    "\n",
    "    #F1= ((data_indices['swir22'] + data_indices['swir16']) - (data_indices['nir'] + data_indices['nir08'])/np.sqrt((data_indices['swir22'] + data_indices['swir16']) + (data_indices['nir'] + data_indices['nir08'])))\n",
    "    #F2= (2-(np.sqrt(data_indices['rededge2'] * data_indices['rededge3'] * (data_indices['nir'] + data_indices['nir08'])/data_indices['red'] + data_indices['rededge1'])))\n",
    "\n",
    "    #data_indices['badi'] = F1 * F2\n",
    "\n",
    "    data_indices['ndwi'] = ((data_indices['green'] - data_indices['nir'])/(data_indices['green'] + data_indices['nir']))\n",
    "\n",
    "    return(data_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_image_stac(bbox,dates):\n",
    "    client = psc.open(URL)\n",
    "    search = client.search(\n",
    "        collections=[collect_amazon],\n",
    "        bbox=bbox,\n",
    "        datetime=dates\n",
    "        )\n",
    "\n",
    "    print(f\"{search.matched()} scenes Sentinel-2 L2A trouvées dans l'intervalle temporel\")\n",
    "    items = search.item_collection()\n",
    "    sentinel_stack = stackstac.stack(\n",
    "                            items,\n",
    "                            bounds_latlon=[bbox[0], bbox[1],  bbox[2],  bbox[3]],\n",
    "\n",
    "                            gdal_env=stackstac.DEFAULT_GDAL_ENV.updated(\n",
    "                                {'GDAL_HTTP_MAX_RETRY': 3,\n",
    "                                    'GDAL_HTTP_RETRY_DELAY': 5,\n",
    "                                }),\n",
    "                            epsg=4326\n",
    "                            ).rename({'x': 'lon', 'y': 'lat'})\n",
    "    data_indices = sentinel_stack.sel(band=[\"blue\",\"rededge1\",\"rededge2\", \"rededge3\", \"green\",\"red\", \"nir\",\"nir08\",\"swir16\",\"swir22\", \"scl\"]).to_dataset(dim='band')\n",
    "    return(data_indices,sentinel_stack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mask_poly(geometry, data_indices):\n",
    "    ba_test_filter = gpd.GeoDataFrame({'geometry': [geometry]}, crs='EPSG:4326', index=[0])\n",
    "\n",
    "    shapes = ba_test_filter['geometry'].apply(mapping)\n",
    "    \n",
    "    # Création du masque avec rasterio\n",
    "    ShapeMask = rasterio.features.geometry_mask(\n",
    "        shapes,\n",
    "        out_shape=(len(data_indices.lat), len(data_indices.lon)),\n",
    "        transform=data_indices.transform,\n",
    "        invert=True\n",
    "    )\n",
    "    \n",
    "    # Convertir le masque en DataArray de xarray\n",
    "    ShapeMask = xr.DataArray(ShapeMask, dims=(\"lat\", \"lon\"))\n",
    "    return ShapeMask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_partition(partition):\n",
    "    row = partition.iloc[0]\n",
    "    date_ = pd.to_datetime(row['date_'], format='%Y-%m-%d').date()\n",
    "    \n",
    "    bbox = row[\"geometry\"].bounds\n",
    "    datemin = (date_ - timedelta(days=interval_before)).strftime('%Y-%m-%d')\n",
    "    datemax = (date_ + timedelta(days=interval_after)).strftime('%Y-%m-%d')\n",
    "    dates = f'{datemin}/{datemax}'\n",
    "\n",
    "    data_indices, sentinel_stack = find_image_stac(bbox, dates)\n",
    "    images_to_keep = []\n",
    "    for i, time in enumerate(pd.to_datetime(sentinel_stack['time']).date):\n",
    "        if time == date_:\n",
    "            images_to_keep.append(i)\n",
    "        else:\n",
    "            scl_data = sentinel_stack.isel(time=i).sel(band=\"scl\").values  \n",
    "            cloud_classes = [3, 8, 9, 10, 11]\n",
    "            cloud_mask = np.isin(scl_data, cloud_classes)\n",
    "            cloud_coverage = np.sum(cloud_mask) / scl_data.size\n",
    "            \n",
    "            if cloud_coverage == 0:\n",
    "                images_to_keep.append(i)\n",
    "\n",
    "    data_to_keep = sentinel_stack.isel(time=images_to_keep)\n",
    "    data_indices = data_to_keep.sel(band=[\"blue\", \"rededge1\", \"rededge2\", \"rededge3\", \"green\", \"red\", \"nir\", \"nir08\", \"swir16\", \"swir22\", \"scl\"]).to_dataset(dim='band')\n",
    "    \n",
    "    data_indices = create_indicateur_spectraux(data_indices)\n",
    "    ShapeMask = create_mask_poly(row['geometry'], data_indices)\n",
    "    data_indices['mask'] = ShapeMask\n",
    "\n",
    "    dataset_save = data_indices.drop_vars([c for c in data_indices.coords if not (c in ['time', 'lat', 'lon'])])\n",
    "\n",
    "    dataset_save = dataset_save.drop_vars([v for v in dataset_save.data_vars if not (v in ['ndvi','badi','ndwi', 'nbr', 'nbr+', 'bais2', 'mask'])])\n",
    "    dataset_save.attrs['spec'] = str(dataset_save.attrs['spec'])\n",
    "\n",
    "    return dataset_save, row['surface_id_h3'], row['date_']\n",
    "\n",
    "nb_line = len(gdf)\n",
    "\n",
    "if offset >= 0 and limit > 0:    \n",
    "    while offset < nb_line:\n",
    "        upper_bound = min(offset + limit, nb_line)\n",
    "        dask_gdf = dask_geopandas.from_geopandas(gdf.iloc[offset:upper_bound], npartitions=12)\n",
    "        result = dask_gdf.map_partitions(process_partition, meta=pd.DataFrame({\"success\": []}, dtype=bool))\n",
    "\n",
    "        result_computed = result.compute()\n",
    "\n",
    "        for ds, surface_id_h3, date_ in result_computed:\n",
    "            filename = f'{surface_id_h3}_{date_}.nc'\n",
    "            output_path = \"A:/INDICATEUR_FEUX/2024/\"\n",
    "\n",
    "            full_path = os.path.join(output_path, filename)\n",
    "            \n",
    "            os.environ['HDF5_USE_FILE_LOCKING'] = 'FALSE'\n",
    "            \n",
    "            ds.to_netcdf(full_path)\n",
    "            os.chmod(full_path, 0o666) \n",
    "\n",
    "            print(f'Saved {filename}')\n",
    "\n",
    "        offset += limit"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
